{"question": "How do I check network connectivity in GPFS?", "answer": "Use the mmdiag --network command to check network connectivity and pending operations in GPFS. This displays diagnostic information about the network state on the current node."}
{"question": "What does mmdiag --iohist show?", "answer": "The mmdiag --iohist command lists the last 512 I/O operations performed by GPFS on the current node. This is useful for diagnosing I/O performance issues and understanding recent file system activity."}
{"question": "How do I collect debug data from a GPFS cluster?", "answer": "Use the gpfs.snap command to collect debug data. This script gathers all logs and configurations from nodes. If you suspect a deadlock, run 'gpfs.snap -deadlock' to collect additional deadlock-specific information."}
{"question": "What is mmfsadm used for in GPFS?", "answer": "mmfsadm is a diagnostic tool intended for trained service personnel. It extracts data from GPFS without using locking, allowing data collection even during locking errors. Common uses include: 'mmfsadm dump waiters' to find long-lasting processes, 'mmfsadm dump all' for comprehensive debug dumps, and 'mmfsadm dump deadlock' for deadlock detection."}
{"question": "How do I check the health of a GPFS cluster?", "answer": "Use the mmhealth command for health monitoring. It creates alerts based on callhome data for recent findings. You can run 'mmhealth node show' to see the health status of nodes, or 'mmhealth node show --unhealthy' to only show unhealthy nodes."}
{"question": "How do I enable tracing in GPFS?", "answer": "Use the mmtracectl command to set up and enable tracing. Trace levels range from 0 to 14, with higher numbers providing more detail. Use 'mmtracectl --set' to configure tracing and 'mmfsadm showtrace' to display current trace levels."}
{"question": "How do I check the state of GPFS daemons on nodes?", "answer": "Use the mmgetstate command to check the GPFS daemon state on nodes. This shows whether the GPFS daemon (mmfsd) is active, down, or in an arbitrating state on each node in the cluster."}
{"question": "How do I list GPFS cluster configuration?", "answer": "Use mmlsconfig to list the cluster configuration parameters. This displays all configuration settings for the GPFS cluster including network settings, timeouts, and performance tuning parameters."}
{"question": "How do I display GPFS cluster information?", "answer": "Use the mmlscluster command to display cluster information including the cluster name, cluster ID, primary and secondary configuration servers, and a list of all nodes in the cluster."}
{"question": "How do I test network performance in a GPFS cluster?", "answer": "Use nsdperf, IBM's network performance tool located in /usr/lpp/mmfs/samples/net. Unlike iperf which tests between two nodes, nsdperf simulates GPFS NSD client/server operations and can coordinate tests across multiple nodes in a large cluster configuration."}
{"question": "What tools can I use to benchmark GPFS performance?", "answer": "Common benchmarking tools for GPFS include: 1) IOR - a parallel IO benchmark for testing storage systems with various interfaces and access patterns, 2) mdtest - tests peak metadata rates including mkdir, stat, rmdir, creat, open, close, and unlink operations, 3) fio - validates infrastructure with configurable block sizes and I/O patterns."}
{"question": "How do I deploy the IBM Spectrum Scale CSI driver?", "answer": "Deploy the IBM Spectrum Scale CSI driver using the operator pattern. Install the CSI operator from OperatorHub or using kubectl/oc. Then create a CSIScaleOperator custom resource that specifies your cluster configuration, including the primary and GUI clusters, secret references for authentication, and node selectors for driver placement."}
{"question": "What are the prerequisites for IBM Spectrum Scale CSI driver?", "answer": "Prerequisites include: 1) A running IBM Spectrum Scale cluster with GUI enabled, 2) Kubernetes 1.19+ or OpenShift 4.6+, 3) The Spectrum Scale client installed on all Kubernetes worker nodes, 4) Network connectivity between Kubernetes nodes and the Spectrum Scale cluster, 5) A Kubernetes secret containing the Scale GUI credentials."}
{"question": "How do I create a PersistentVolumeClaim with IBM Spectrum Scale CSI?", "answer": "Create a StorageClass referencing the CSI driver (spectrumscale.csi.ibm.com), then create a PVC referencing that StorageClass. The CSI driver will dynamically provision a fileset on your Spectrum Scale filesystem. Example: Create a StorageClass with 'provisioner: spectrumscale.csi.ibm.com' and configure volBackendFs to specify which filesystem to use."}
{"question": "How do I diagnose a GPFS deadlock?", "answer": "To diagnose a GPFS deadlock: 1) Run 'gpfs.snap -deadlock' to collect deadlock-specific debug data, 2) Use 'mmfsadm dump deadlock' for deadlock detection, 3) Check 'mmfsadm dump waiters' to find long-lasting processes that may be blocking others. The collected data can be analyzed by IBM support."}
{"question": "What should I check when GPFS nodes are being expelled from the cluster?", "answer": "When nodes are expelled: 1) Check mmfs.log for the reason - in GPFS 6.0.0+, the log provides clearer details about expel decisions, 2) Use mmhealth to see expel reports via the postExpel callback, 3) Common causes include network issues (check with mmnetverify), lease expiration, or inter-node RPC failures."}
{"question": "How do I automate IBM Storage Scale installation?", "answer": "IBM provides several automation options: 1) Ansible playbooks for automated installation, configuration, verification, and upgrades, 2) Terraform modules for cloud provisioning in AWS, Azure, and IBM Cloud, 3) Vagrant with StorageScaleVagrant for development/test environments. The Installation Toolkit also provides automated deployment capabilities."}
{"question": "What is IBM Storage Scale Container Native?", "answer": "IBM Storage Scale Container Native provides native integration of Spectrum Scale with Kubernetes/OpenShift environments. It allows containers to directly access Spectrum Scale filesystems with enterprise features like snapshots, quotas, and data tiering. It's deployed alongside the CSI driver for complete container storage integration."}
{"question": "How do I monitor IBM Storage Scale with Grafana?", "answer": "Use the IBM Spectrum Scale Bridge for Grafana. This bridge collects performance metrics from your Scale cluster and exposes them to Prometheus, which Grafana can then visualize. Install the bridge, configure it to connect to your Scale cluster's performance monitoring (mmperfmon), and import the provided Grafana dashboards."}
{"question": "What metrics are available for IBM Storage Scale monitoring?", "answer": "IBM Storage Scale exposes metrics through mmperfmon including: filesystem throughput (read/write MB/s), IOPS, latency, metadata operations, network statistics, disk utilization, and per-fileset metrics. These can be collected via the Grafana bridge or directly queried using mmpmon."}
{"question": "How do I use bridge-for-grafana?", "answer": "With the bridge version 4, the new collection of example dashboards \"Example Dashbords bundle\" has been added to the available for free download resources. These new dashboards could be used for managing a multi-cluster environment. Also the \"Default Dashboards set\" and  \"Advanced Dashboards set\"  have been merged into this download package.\n\nThe package content consists of several folders:\n\n- Predefined Basic Dashboards - including all dashboard examples from \"Default Dashboard set\" package\n- Advanced Dashboards -  including all dashboard examples from \"Advanced Dashboards set\" package\n- HOWTO - including dashboard examples with learning effect, f.e. Grafana's helpful features\n- NamedQueries - including dashboard examples for monitoring Linux(Network) metrics\n- Protocols - including dashboard examples for monitoring SMB and NFS metrics\n- TCT - including dashboard examples for monitoring TCT/cloud data transfers\n\nYou can create the same folder structure in your running Grafana environment via provisioning folders structure from filesystem to Grafana.\nFollow [step-by-step instructions](https://github.com/IBM/ibm-spectrum-scale-bridge-for-grafana/wiki/Make-usage-of-Grafana-Provisioning-feature#provision-the-example-dashboards-bundle-collection-with-folders-structure-to-grafana) available in the project Wiki."}
{"question": "What are the prerequisite steps on remote storage cluster?", "answer": "A number of preleminary tasks must be executed on the IBM Storage Scale storage cluster. \n\nSetup IBM Storage Scale bridge for Grafana with PrometheusExporter enabled.\nEnsure that the node on which PrometheusExporter is running is accessible from the OpenShift cluster.\nCopy the SSL/TLS key and certificate configured for the IBM Storage Scale bridge for Grafana on the remote cluster to the OpenShift cluster."}
{"question": "How do I deploy steps on openshift cluster?", "answer": "Enable monitoring for user-defined projects\n```shell\noc apply -f https://raw.githubusercontent.com/IBM/ibm-spectrum-scale-bridge-for-grafana/refs/heads/master/examples/openshift_deployment_scripts/cnsa_workload_monitoring/cluster-monitoring-config.yml\n```\n\nCreate new project\n```shell\noc new-project ibm-external-storage\n```\n\nCreate tls secret from tls key/certificate files\n```shell\noc create secret tls grafanabridge-external-tls-data --cert=</path/to/cert.crt> --key=</path/to/cert.key> -n ibm-external-storage\n```\n\nOpen external_storage_endpoint.yaml in edit mode and update the ip field with the the ip adress of the remote storage cluster node running PrometheusExporter.\nApply the external_storage_endpoint.yaml\n```shell\noc apply -f external-storage-endpoint.yaml\n```\n\nDeploy ibm-example-external-storage-service Service resource\n```shell\noc apply -f external-storage-endpoint-service.yml\n```\n\nDeploy prometheus-grafanabridge-external-monitor ServiceMonitor resource \n```shell\noc apply -f grafanabridge-external-service-monitor.yml"}
{"question": "How do I use cloud-install?", "answer": "* [Amazon Web Services (AWS)](docs/aws.md)\n* [Microsoft Azure](docs/azure.md)\n* [IBM Cloud](docs/ibmcloud.md)\n\n### Reporting Issues and Feedback\n\nTo file issues, suggestions, new features, etc., please open an [Issue](https://github.com/IBM/ibm-spectrum-scale-cloud-install/issues).\n\n### Disclaimer\n\nPlease note: All templates / modules / resources in this repo are released for use \"AS IS\" without any warranties of\nany kind, including, but not limited to their installation, use, or performance. We are not responsible for any damage,\ndata loss or charges incurred with their use. You are responsible for reviewing and testing any scripts you run\nthoroughly before use in any production environment. This content is subject to change without notice.\n\n### Contribute Code\n\nWe welcome contributions to this project, see [Contributing](CONTRIBUTING.md) for more details."}
{"question": "How do I deploy options?", "answer": "The terraform templates provided in this repository offer following deployment options;\n\n> **_NOTE:_** By default below options use *terraform local backend*.\nIn order to configure below options to use *terraform S3 backend*, use `./tools/enable_aws_s3_backend.sh`.\n\n### (Option-1) New VPC Based Configuration (Single AZ, Multi AZ)\n\nThis option provisions a new AWS VPC environment consisting of the subnets, nat gateways, internet gateway, security groups, bastion autoscaling group, compute (instances with NO EBS volumes attached) and storage (instances with EBS volumes attached) instances, and then deploys IBM Spectrum Scale into this new VPC with a single or multi availability zone(s).\n\nRefer to [New VPC Based Configuration](../aws_scale_templates/aws_new_vpc_scale/README.md) for detailed steps, options.\n\n### (Option-2) Existing VPC Based Configuration (Single AZ, Multi AZ)\n\nThis option deploys IBM Spectrum Scale in to an existing VPC (which can have subnets with multiple availability zones).\n\nRefer [Existing VPC Based Configuration](../aws_scale_templates/sub_modules/instance_template/README.md) for detailed steps, options.\n\n> This mode provides flexibility to bypass bastion/jump host, incase local/cloud VM has direct connectivity to VPC."}
{"question": "What are the requirements?", "answer": "| Name | Version |\n|------|---------|\n| <a name=\"requirement_terraform\"></a> [terraform](#requirement\\_terraform) | ~> 1.0 |\n| <a name=\"requirement_google\"></a> [google](#requirement\\_google) | ~> 4.0.0 |"}
{"question": "What are the 3. prerequisites?", "answer": "- **Delete any existing workloads or application pods attached to the PVCs that will be migrated** from the existing IBM Storage Scale container native cluster.\n\nBefore running the migration script, ensure the following tools are installed and available in your `$PATH`:\n\n- The script should be run in the context of the cluster where IBM Storage Scale container native is deployed\n- **kubectl** \u2013 to interact with the Kubernetes cluster and fetch/update PV/PVC objects\n- **jq** \u2013 for JSON parsing and manipulation of Kubernetes API responses\n\nYou can verify installation with:\n\n```bash\nkubectl version --client\njq --version\n```"}
{"question": "How do I use csi?", "answer": "### Before (PV created with **primary filesystem/fileset** enabled):\n```text\nvolumeHandle: 0;2;13009550825755318848;A3D56F10:9BC12E30;;pvc-3b1a-49d3-89e1-51f607b91234;/ibm/remotefs1/primary-remotefs1-123456789/.volumes/pvc-3b1a-49d3-89e1-51f607b91234\n```\n\n### After (Migrated to **actual fileset mount path**):\n```text\nvolumeHandle: 0;2;13009550825755318848;A3D56F10:9BC12E30;;pvc-3b1a-49d3-89e1-51f607b91234;/var/mnt/remotefs1/fs1-pvc-3b1a-49d3-89e1-51f607b91234/pvc-3b1a-49d3-89e1-51f607b91234-data\n```\n\n### Key Points\n\n**Unchanged:**\n- The identity portion of the handle (everything up to the last `;`).\n\n**Rewritten:**\n- Only the **path segment after the last `;`**, now pointing to the actual fileset mount path."}
{"question": "Can you show an example GrafanaDashboard configuration for IBM Spectrum Scale?", "answer": "Here's an example GrafanaDashboard configuration:\n\n```yaml\n---\napiVersion: grafana.integreatly.org/v1beta1\nkind: GrafanaDashboard\nmetadata:\n  name: openshif-cnsa-cluster-overview\nspec:\n  folder: \"my-folder\"\n  instanceSelector:\n    matchLabels:\n      dashboards: \"my-dashboards\"\n  url: \"https://raw.githubusercontent.com/IBM/ibm-spectrum-scale-bridge-for-grafana/refs/heads/master/examples/grafana_dashboards/cnsa_with_openshift_monitoring/Openshift%20cluster%20and%20IBM%20Storage%20Scale%20cloud%20native%20project%20overview-1746192604852.json\"\n---\napiVersion: grafana.integreatly.org/v1beta1\nkind: GrafanaDashboard\nmetadata:\n  name: cnsa-components-health-overview\nspec:\n  folder: \"my-folder\"\n  instanceSelector:\n    matchLabels:\n      dashboards: \"my-dashboards\"\n  url: \"https://raw.githubusercontent.com/IBM/ibm-spectrum-scale-bridge-for-grafana/refs/heads/master/examples/grafana_dashboards/cnsa_with_openshift_monitoring/Component%20health%20view%20by%20node%20selection%20for%20IBM%20Storage%20Scale%20container%20native-1746192777493.json\"\n---\napiVersion: grafana.integreatly.org/v1beta1\nkind: GrafanaDashboard\nmetadata:\n  name: cnsa-component-entities-overview\nspec:\n  folder: \"my-folder\"\n  instanceSelector:\n    matchLabels:\n      dashboards: \"my-d\n```"}
